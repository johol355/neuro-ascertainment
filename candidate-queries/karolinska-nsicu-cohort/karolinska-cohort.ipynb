{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Define the SQLalchemy engine\n",
    "engine = create_engine(f\"sqlite:////Users/JO/PhD/neuro-ascertainment/data/db.sqlite\")\n",
    "\n",
    "# Read the SQL query from the file\n",
    "with open('/Users/JO/PhD/neuro-ascertainment/candidate-queries/karolinska-nsicu-cohort/karolinska-cohort.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "\n",
    "SEED = 20\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def count_id(df):\n",
    "    LopNr = df['LopNr'].nunique() if 'LopNr' in df else 'Column missing'\n",
    "    VtfId_LopNr = df['VtfId_LopNr'].nunique() if 'VtfId_LopNr' in df else 'Column missing'\n",
    "    HADM_ID = df['HADM_ID'].nunique() if 'HADM_ID' in df else 'Column missing'\n",
    "    return print(f'Unique patients: {LopNr} | Unique SIR admits: {VtfId_LopNr} | Unique PAR admits: {HADM_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAR_HADM\n",
    "PAR_HADM contains all PAR admits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 59333 | Unique SIR admits: Column missing | Unique PAR admits: 359305\n"
     ]
    }
   ],
   "source": [
    "query_PAR_HADM = query + \"SELECT * FROM PAR_HADM\"\n",
    "PAR_HADM = pd.read_sql(query_PAR_HADM, engine)\n",
    "count_id(PAR_HADM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_ICU_ADMISSIONS\n",
    "All ICU admissions at K CIVA and K NIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 6454 | Unique SIR admits: 7673 | Unique PAR admits: Column missing\n"
     ]
    }
   ],
   "source": [
    "query_K_ICU_ADMISSIONS = query + \"SELECT * FROM K_ICU_ADMISSIONS\"\n",
    "K_ICU_ADMISSIONS = pd.read_sql(query_K_ICU_ADMISSIONS, engine)\n",
    "count_id(K_ICU_ADMISSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_ICU_ADMISSIONS_MATCHED_WITH_PAR\n",
    "Left join PAR admissions (with certain criteria) on K_ICU_ADMISSIONS on LopNr (patient ID) . Approx 800 patients are lost at this step as they do not have a proper patient ID (\"reservnummer\").\n",
    "\n",
    "Note that each patient can have several ICU admits here. Also, each ICU admit can be matched with several PAR admits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 5673 | Unique SIR admits: 6539 | Unique PAR admits: 7141\n"
     ]
    }
   ],
   "source": [
    "query_K_ICU_ADMISSIONS_MATCHED_WITH_PAR = query + \"SELECT * FROM K_ICU_ADMISSIONS_MATCHED_WITH_PAR\"\n",
    "K_ICU_ADMISSIONS_MATCHED_WITH_PAR = pd.read_sql(query_K_ICU_ADMISSIONS_MATCHED_WITH_PAR, engine)\n",
    "count_id(K_ICU_ADMISSIONS_MATCHED_WITH_PAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX\n",
    "Inferred diagnosis for the PAR admit is added to K_ICU_ADMISSIONS_MATCHED_WITH_PAR. In a few cases the criteria for several DX are fulfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 5673 | Unique SIR admits: 6539 | Unique PAR admits: 7141\n"
     ]
    }
   ],
   "source": [
    "query_K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX = query + \"SELECT * FROM K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX\"\n",
    "K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX = pd.read_sql(query_K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX, engine)\n",
    "count_id(K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX_TIME_HIERARCHY\n",
    "This takes care of situations where a ICU admit is matched with several PAR admits. Here a column called DX_ORDER is introduced, ranking PAR admits based on \"relevancy\". The earliest admit is most relevant. If there is a tie on date, a diagnostic group hierarchy is used to decide rank. All rows with \"OTHER\" dx are removed, that's why we loose patients in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 4150 | Unique SIR admits: 4806 | Unique PAR admits: 4965\n"
     ]
    }
   ],
   "source": [
    "query_TIME_HIERARCHY= query + \"SELECT * FROM K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX_TIME_HIERARCHY\"\n",
    "TIME_HIERARCHY  = pd.read_sql(query_TIME_HIERARCHY, engine)\n",
    "count_id(TIME_HIERARCHY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2561574566.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['INDATUM'] = pd.to_datetime(small_d['INDATUM'], unit='D')\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2561574566.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['UTDATUM'] = pd.to_datetime(small_d['UTDATUM'], unit='D')\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2561574566.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['InskrTidPunkt'] = pd.to_datetime(small_d['InskrTidPunkt'], unit='s').dt.normalize()\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2561574566.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['UtskrTidPunkt'] = pd.to_datetime(small_d['UtskrTidPunkt'], unit='s').dt.normalize()\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2561574566.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['timediff_to_sir'] = filtered_df['INDATUM'] - filtered_df['InskrTidPunkt']\n"
     ]
    }
   ],
   "source": [
    "small_d = TIME_HIERARCHY[['VtfId_LopNr','InskrTidPunkt', 'UtskrTidPunkt', 'HADM_ID', 'MVO', 'LopNr', 'INDATUM', 'UTDATUM', 'DX_GROUP', 'DIAGNOS', 'DX_ORDER']]\n",
    "small_d['INDATUM'] = pd.to_datetime(small_d['INDATUM'], unit='D')\n",
    "small_d['UTDATUM'] = pd.to_datetime(small_d['UTDATUM'], unit='D')\n",
    "small_d['InskrTidPunkt'] = pd.to_datetime(small_d['InskrTidPunkt'], unit='s').dt.normalize()\n",
    "small_d['UtskrTidPunkt'] = pd.to_datetime(small_d['UtskrTidPunkt'], unit='s').dt.normalize()\n",
    "\n",
    "filtered_df = small_d[small_d.groupby('VtfId_LopNr')['VtfId_LopNr'].transform('size') > 1]\n",
    "filtered_df['timediff_to_sir'] = filtered_df['INDATUM'] - filtered_df['InskrTidPunkt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VtfId_LopNr</th>\n",
       "      <th>InskrTidPunkt</th>\n",
       "      <th>UtskrTidPunkt</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>MVO</th>\n",
       "      <th>LopNr</th>\n",
       "      <th>INDATUM</th>\n",
       "      <th>UTDATUM</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DIAGNOS</th>\n",
       "      <th>DX_ORDER</th>\n",
       "      <th>timediff_to_sir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>181018.0</td>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>117779</td>\n",
       "      <td>301</td>\n",
       "      <td>21159.0</td>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>2021-09-03</td>\n",
       "      <td>TBI</td>\n",
       "      <td>S066 S066 S0630 S1210 S0210 S001 S0650 I620</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>181018.0</td>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>117780</td>\n",
       "      <td>331</td>\n",
       "      <td>21159.0</td>\n",
       "      <td>2021-09-03</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>TBI</td>\n",
       "      <td>S066 S066 S1210 S1210 S001 S001 S0650 S0650 S0...</td>\n",
       "      <td>2</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      VtfId_LopNr InskrTidPunkt UtskrTidPunkt  HADM_ID  MVO    LopNr  \\\n",
       "5201     181018.0    2021-09-02    2021-10-05   117779  301  21159.0   \n",
       "5202     181018.0    2021-09-02    2021-10-05   117780  331  21159.0   \n",
       "\n",
       "        INDATUM    UTDATUM DX_GROUP  \\\n",
       "5201 2021-09-02 2021-09-03      TBI   \n",
       "5202 2021-09-03 2021-10-05      TBI   \n",
       "\n",
       "                                                DIAGNOS  DX_ORDER  \\\n",
       "5201        S066 S066 S0630 S1210 S0210 S001 S0650 I620         1   \n",
       "5202  S066 S066 S1210 S1210 S001 S001 S0650 S0650 S0...         2   \n",
       "\n",
       "     timediff_to_sir  \n",
       "5201          0 days  \n",
       "5202          1 days  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_vtf = rng.choice(filtered_df['VtfId_LopNr'], 1)\n",
    "filtered_df.query(f\"VtfId_LopNr == {check_vtf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX_HIERARCHY_TIME\n",
    "Same thing as above, but hierarchy first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 4150 | Unique SIR admits: 4806 | Unique PAR admits: 4965\n"
     ]
    }
   ],
   "source": [
    "query_HIERARCHY_TIME= query + \"SELECT * FROM K_ICU_ADMISSIONS_MATCHED_WITH_PAR_WITH_DX_HIERARCHY_TIME\"\n",
    "HIERARCHY_TIME  = pd.read_sql(query_HIERARCHY_TIME, engine)\n",
    "count_id(HIERARCHY_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2839594063.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['INDATUM'] = pd.to_datetime(small_d['INDATUM'], unit='D')\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2839594063.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['UTDATUM'] = pd.to_datetime(small_d['UTDATUM'], unit='D')\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2839594063.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['InskrTidPunkt'] = pd.to_datetime(small_d['InskrTidPunkt'], unit='s').dt.normalize()\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2839594063.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  small_d['UtskrTidPunkt'] = pd.to_datetime(small_d['UtskrTidPunkt'], unit='s').dt.normalize()\n",
      "/var/folders/s_/hnbjtb757x10k0pyf6_x_9mw0000gn/T/ipykernel_21723/2839594063.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['timediff_to_sir'] = filtered_df['INDATUM'] - filtered_df['InskrTidPunkt']\n"
     ]
    }
   ],
   "source": [
    "small_d = HIERARCHY_TIME[['VtfId_LopNr','InskrTidPunkt', 'UtskrTidPunkt', 'HADM_ID', 'MVO', 'LopNr', 'INDATUM', 'UTDATUM', 'DX_GROUP', 'DIAGNOS', 'DX_ORDER']]\n",
    "small_d['INDATUM'] = pd.to_datetime(small_d['INDATUM'], unit='D')\n",
    "small_d['UTDATUM'] = pd.to_datetime(small_d['UTDATUM'], unit='D')\n",
    "small_d['InskrTidPunkt'] = pd.to_datetime(small_d['InskrTidPunkt'], unit='s').dt.normalize()\n",
    "small_d['UtskrTidPunkt'] = pd.to_datetime(small_d['UtskrTidPunkt'], unit='s').dt.normalize()\n",
    "\n",
    "filtered_df = small_d[small_d.groupby('VtfId_LopNr')['VtfId_LopNr'].transform('size') > 1]\n",
    "filtered_df['timediff_to_sir'] = filtered_df['INDATUM'] - filtered_df['InskrTidPunkt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VtfId_LopNr</th>\n",
       "      <th>InskrTidPunkt</th>\n",
       "      <th>UtskrTidPunkt</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>MVO</th>\n",
       "      <th>LopNr</th>\n",
       "      <th>INDATUM</th>\n",
       "      <th>UTDATUM</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DIAGNOS</th>\n",
       "      <th>DX_ORDER</th>\n",
       "      <th>timediff_to_sir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>132411.0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>2014-02-14</td>\n",
       "      <td>68368</td>\n",
       "      <td>331</td>\n",
       "      <td>12135.0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>2014-02-16</td>\n",
       "      <td>ABM</td>\n",
       "      <td>G060 I489 I109 Z958 I340 J324</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>132411.0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>2014-02-14</td>\n",
       "      <td>68370</td>\n",
       "      <td>121</td>\n",
       "      <td>12135.0</td>\n",
       "      <td>2014-02-20</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>ABM</td>\n",
       "      <td>G060</td>\n",
       "      <td>2</td>\n",
       "      <td>10 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      VtfId_LopNr InskrTidPunkt UtskrTidPunkt  HADM_ID  MVO    LopNr  \\\n",
       "1796     132411.0    2014-02-10    2014-02-14    68368  331  12135.0   \n",
       "1797     132411.0    2014-02-10    2014-02-14    68370  121  12135.0   \n",
       "\n",
       "        INDATUM    UTDATUM DX_GROUP                        DIAGNOS  DX_ORDER  \\\n",
       "1796 2014-02-10 2014-02-16      ABM  G060 I489 I109 Z958 I340 J324         1   \n",
       "1797 2014-02-20 2014-03-24      ABM                           G060         2   \n",
       "\n",
       "     timediff_to_sir  \n",
       "1796          0 days  \n",
       "1797         10 days  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_vtf = rng.choice(filtered_df['VtfId_LopNr'], 1)\n",
    "filtered_df.query(f\"VtfId_LopNr == {check_vtf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY_TABLE\n",
    "Finally, using the time and dx hierarchy as tiebreakers. We choose the first ICU admit for each patient and match it with a PAR admit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_DESC_PAR = query + \"SELECT * FROM SUMMARY_TABLE\"\n",
    "DESC = pd.read_sql(query_DESC_PAR, engine)\n",
    "DESC\n",
    "\n",
    "DESC['par_adm_date'] = pd.to_datetime(DESC['par_adm_date'], unit='D')\n",
    "DESC['par_dsc_date'] = pd.to_datetime(DESC['par_dsc_date'], unit='D')\n",
    "DESC['sir_adm_time'] = pd.to_datetime(DESC['sir_adm_time'], unit='s').dt.normalize()\n",
    "DESC['sir_dsc_time'] = pd.to_datetime(DESC['sir_dsc_time'], unit='s').dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients: 4150 | Unique SIR admits: 4150 | Unique PAR admits: 4150\n"
     ]
    }
   ],
   "source": [
    "count_id(DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sir_total_time</th>\n",
       "      <th>SAPS_GCS</th>\n",
       "      <th>any_AMV</th>\n",
       "      <th>d30</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASAH</th>\n",
       "      <td>57.619849</td>\n",
       "      <td>0.645080</td>\n",
       "      <td>8436.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.637511</td>\n",
       "      <td>0.124474</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBI</th>\n",
       "      <td>54.500000</td>\n",
       "      <td>0.256604</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.790566</td>\n",
       "      <td>0.183019</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICH</th>\n",
       "      <td>58.551127</td>\n",
       "      <td>0.407279</td>\n",
       "      <td>4143.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.856153</td>\n",
       "      <td>0.344887</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIS</th>\n",
       "      <td>63.350711</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>2781.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.850711</td>\n",
       "      <td>0.281991</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFX</th>\n",
       "      <td>60.633858</td>\n",
       "      <td>0.255906</td>\n",
       "      <td>3502.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>0.098425</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABM</th>\n",
       "      <td>54.734104</td>\n",
       "      <td>0.491329</td>\n",
       "      <td>4606.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.780347</td>\n",
       "      <td>0.121387</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TUM</th>\n",
       "      <td>51.262295</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>2944.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.762295</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEP</th>\n",
       "      <td>55.666667</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>1317.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDH</th>\n",
       "      <td>60.194444</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>2805.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HC</th>\n",
       "      <td>55.674419</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENC</th>\n",
       "      <td>47.205882</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>6112.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVM</th>\n",
       "      <td>51.062500</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>2870.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVT</th>\n",
       "      <td>44.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4047.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  sex_female  sir_total_time  SAPS_GCS   any_AMV       d30  \\\n",
       "DX_GROUP                                                                        \n",
       "ASAH      57.619849    0.645080          8436.0      14.0  0.637511  0.124474   \n",
       "TBI       54.500000    0.256604          3000.0      11.0  0.790566  0.183019   \n",
       "ICH       58.551127    0.407279          4143.0       8.0  0.856153  0.344887   \n",
       "AIS       63.350711    0.334123          2781.0      10.0  0.850711  0.281991   \n",
       "CFX       60.633858    0.255906          3502.5      15.0  0.700787  0.098425   \n",
       "ABM       54.734104    0.491329          4606.0      12.0  0.780347  0.121387   \n",
       "TUM       51.262295    0.442623          2944.5      11.0  0.762295  0.180328   \n",
       "SEP       55.666667    0.391667          1317.5      10.0  0.808333  0.016667   \n",
       "SDH       60.194444    0.305556          2805.0      12.0  0.712963  0.250000   \n",
       "HC        55.674419    0.488372          3520.0      11.0  0.837209  0.093023   \n",
       "ENC       47.205882    0.352941          6112.5      10.0  0.794118  0.029412   \n",
       "AVM       51.062500    0.531250          2870.0      15.0  0.562500  0.062500   \n",
       "CVT       44.375000    0.500000          4047.5      11.0  0.500000  0.187500   \n",
       "\n",
       "             n  \n",
       "DX_GROUP        \n",
       "ASAH      1189  \n",
       "TBI       1060  \n",
       "ICH        577  \n",
       "AIS        422  \n",
       "CFX        254  \n",
       "ABM        173  \n",
       "TUM        122  \n",
       "SEP        120  \n",
       "SDH        108  \n",
       "HC          43  \n",
       "ENC         34  \n",
       "AVM         32  \n",
       "CVT         16  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = DESC.groupby('DX_GROUP').agg({'age': 'mean', 'sex_female': 'mean', 'sir_total_time': 'median', 'SAPS_GCS': 'median', 'any_AMV': 'mean', 'd30': 'mean', 'VtfId_LopNr': 'count'})\n",
    "s.rename(columns={'VtfId_LopNr': 'n'}).sort_values(by='n', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epidemiology-pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
